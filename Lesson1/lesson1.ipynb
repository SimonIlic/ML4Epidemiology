{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: My First Neural Network (Diabetes Prediction)\n",
    "\n",
    "## Preamble: What is this?\n",
    "This is a Jupyter Notebook. It is like a physical notebook with text and images, but it also runs python scripts. We are hosting this notebook in the cloud on Google Colab. Later, if you are using Machine Learning algorithms in your own research you will probably run python scripts locally on your own computer, or perhaps even a super-computer. For learning however a notebook can be useful. It allows us to provide helpful information besides all the code. Running it on Google Colab allows us to get started without any installation burdens on your local device.\n",
    "\n",
    "In  a notebook there are two types of fields, a text field (in markdown) like this one. And a code field, with python code, like the one below here. Python is a programming language like R. It can run algorithms and do calculations. You can run a code cell using the play button (or ctrl+enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also comes with many add-ons, prefabricated libraries of code, known as modules. These modules make doing complex things simple. We can import them using `import`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example the datetime module provides a bunch of code to work with dates and times\n",
    "import datetime\n",
    "datetime.date.today() + datetime.timedelta(days=7)  # this makes time arithmetic easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this course we will be working with a bunch of famous machine learning modules, let's import them now. We will get into what they all do later, once we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, pandas, torch, sklearn, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Predicting Diabetes\n",
    "\n",
    "For this class we will be training a neural network to predict diabetes based on factors like blood pressure, insulin and glucose levels. In order to train a model and make predictions, first we will need some data. We need data both to train a model, training data, as well as to test its performance, test data.\n",
    "\n",
    "In this notebook we will be using the [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "The datasets consist of several medical predictor variables and one target variable, Outcome. Predictor variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Exploring the dataset\n",
    "Let's load the dataset from the web. See if you can make sense from these lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is as a Comma Seperated Values (csv) file hosted on GitHub at this URL\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "# we declase the column names since the csv file does not have a header row\n",
    "headers = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DPF', 'Age', 'Diabetes']\n",
    "# we use pandas to read the csv file into a DataFrame, which is a tabular data structure with rows and columns\n",
    "dataset = pandas.read_csv(url, names=headers)\n",
    "dataset.head()  # df.head() shows the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our dataset as a [Pandas](https://pandas.pydata.org/) Dataframe. A dataframe is like a table, but more dynamic and malleable. We can do things like sorting and filtering the table, just like in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by age\n",
    "dataset.sort_values(by='Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rows where glucose > 120\n",
    "dataset[dataset['Glucose'] > 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get some stats\n",
    "mean_insulin = dataset.Insulin.mean()  # mean insulin level\n",
    "max_bmi = dataset.BMI.max()  # max BMI\n",
    "print(f\"Mean insulin level: {mean_insulin}, Max BMI: {max_bmi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Make yourself familiar with the datset.\n",
    "a) To get yourself familiar with the dataset let's explore the dataframe a bit. For each column write down what it signifies, including units. You will have to look this up online.\n",
    "\n",
    "* Pregnancies: Total number of times pregnant (#)\n",
    "* Glucose: Plasma glucose concentration (2-hour OGTT) (mg/dL)\n",
    "* BloodPressure: ... (..)\n",
    "* SkinThickness: ... (..)\n",
    "* Insulin: ... (..)\n",
    "* BMI: ... (..)\n",
    "* DPF: ... (..)\n",
    "* Age: ... (..)\n",
    "* Diabetes: ... (..)\n",
    "\n",
    "b) Explore the dataset to answer the following questions:\n",
    "1. What is the maximum number of pregnancies a woman endured in the dataset?\n",
    "2. Who is the mean age of the individuals in the dataset?\n",
    "3. How many people have diabetes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free Code Field for exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your answers to 1b) here\n",
    "max_pregnancies = ...\n",
    "mean_age = ...\n",
    "num_diabetes = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like you can check your answers using this line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line of code is obfuscated by design, no need to understand it, just run it to check your answers\n",
    "import answers; assert max_pregnancies == answers.max_pregnancies, \"Something is wrong with your answer for max_pregnancies\"; numpy.testing.assert_approx_equal(mean_age, answers.mean_age); assert num_diabetes == answers.num_diabetes, \"Something is wrong with your answer for num_diabetes\"; print(\"All answers are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our data through histograms. Run the line below to display 9 histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(figsize=(12, 10), bins=20); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Based on what you know now about the dataset, what columns should be the features X, and what column should be the target y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"...\"\n",
    "X = dataset.drop(columns=[target_column]).values\n",
    "y = dataset[target_column].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in class we need to split our data into a train and a test set. Normally we would also need a validation set to test your model during prototyping. Since this is not a real experiment, we will keep things simple and split only into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important that data is normalized before running any deep learning model fitting. Neural Networks prefer standard normal data. For this we will use a rescaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(scaler.fit_transform(X_train))\n",
    "X_test = torch.FloatTensor(scaler.transform(X_test))\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot that data again. What do you notice? What happened to the 9th histogram? Did all our normalization operations succeed? What are the spikes we see? If you want you can take some time to improve your data pre-processing pipeline and use these plots to check their effect. Once you feel ready, move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms\n",
    "pandas.DataFrame(X_train).hist(figsize=(12, 10), bins=20); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network\n",
    "\n",
    "Let's get to the heart of this notebook: We are going to build a model to make predictions: The Multi-Layer Perceptron. This is a fancy term for a standard feed-forward neural network.\n",
    "\n",
    "Think of a neural network like a committee of doctors looking at a patient's chart:\n",
    "\n",
    "1.  **Input Layer:** These are the numbers on the chart (Glucose, Age, BMI, etc.). We have 8 charts (features), so our input layer needs 8 nodes.\n",
    "2.  **Hidden Layer:** These are the junior doctors. They look at combinations of the inputs to find patterns (e.g., \"High age + High BMI\" might be a specific risk factor). We will use **12 neurons** here. Why 12? It's a design choice; enough to learn patterns, but not so big that it memorizes the data.\n",
    "3.  **Activation Function (ReLU):** This introduces non-linearity. Without it, the network is just a linear regression line. It decides if a neuron \"fires\" or stays silent.\n",
    "4.  **Output Layer:** This is the head doctor. They take the insights from the junior doctors and make a final recommendation: How confident are we the patient has diabetes? \n",
    "5.  **Output Activation (Sigmoid):** This squashes the final number into a probability between 0 and 1.\n",
    "\n",
    "We will build our model using the pytorch module. PyTorch is an open-source machine learning framework for building and training neural networks using Python. It behaves very similar to numpy, making models easy to write, debug, and experiment with. PyTorch is widely used in both research and production and integrates tightly with GPUs and modern deep learning tooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DiabetesClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, n_hidden, output_dim):\n",
    "        super(DiabetesClassifier, self).__init__()\n",
    "        # these are all the building blocks of our network\n",
    "        self.hidden_layer = nn.Linear(input_dim, n_hidden) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(n_hidden, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden_layer(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our model architecture. Let's build it. Defining our model requires 3 parameters: The input dimension, number of hidden neurons and output dimension. Some of these are set by the data we are dealing with, some are free design choices. Based on what you learned in class try to draw out the neural network we defined above. What are the number of input nodes, hidden nodes and output nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = ...\n",
    "n_hidden = ...  # this is a free hyperparameter, you can change it to whatever you see fit\n",
    "output_dim = ...\n",
    "\n",
    "model = neural_network = DiabetesClassifier(input_dim, n_hidden, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test you understanding of the model, how many configurable parameters (or weights) does this model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_parameters = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check you answer by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert total_number_of_parameters == answers.num_of_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a model, but it is currently \"dumb\"â€”it has not learned anything yet. To train it, we need two things:\n",
    "\n",
    "1.  **Criterion (Loss Function):** A way to measure how wrong the model is at the current training step. Since we are doing binary classification (0 or 1), we use **Binary Cross Entropy (BCELoss)**. If the model predicts 0.9 (Diabetes) but the true answer is 0 (No Diabetes), the Loss will be high. If the model predicts 0.1 and the true answer is 0, the loss will be low.\n",
    "2.  **Optimizer:** A way to update the model to make it less wrong next time. We will use **Adam**, a very popular optimizer that adjusts the learning rate automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Process\n",
    "\n",
    "This is where the magic happens. We will force the model to look at the data 100 times (100 **Epochs**). In every single epoch, the following steps happen:\n",
    "\n",
    "1.  **Forward Pass:** The model makes a guess based on it's current weights.\n",
    "2.  **Calculate Loss:** We check how wrong that guess was compared to `y_train`.\n",
    "3.  **Backward Pass (Backpropagation):** We calculate which weights contributed to the error.\n",
    "4.  **Step:** The optimizer nudges the weights in the opposite direction of the error to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "eval_losses = []\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.plot(losses, label='Train Loss')\n",
    "plt.title(\"Loss Curve (The Mechanics of Learning)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your time now to play around with the free hyperparameters, and see how they adjust our training dynamics. Make sure to re-run the entire notebook after you adjust something using the Run All button up above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Now that we have trained our model, let's make some predictions! Play around with this datapoint and see how to model's predictions change based on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pregnancies = 3\n",
    "glucose = 80\n",
    "blood_pressure = 70\n",
    "skin_thickness = 20\n",
    "insulin = 79.0\n",
    "bmi = 32.0\n",
    "dpf = 0.5\n",
    "age = 21\n",
    "\n",
    "patient_info = torch.FloatTensor([pregnancies, glucose, blood_pressure, skin_thickness, insulin, bmi, dpf, age])\n",
    "# don't forget to scale the datapoint!\n",
    "patient_info = scaler.transform(patient_info.view(1, -1))\n",
    "\n",
    "prediction = model(torch.FloatTensor(patient_info)).item()\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained! But how well does it diagnose patients it has never seen before? We held back `X_test` for this exact moment.\n",
    "\n",
    "We need to:\n",
    "1. Turn off the gradient calculation (we aren't training anymore, just testing).\n",
    "2. Pass the test data through the model.\n",
    "3. Round the results (since the model outputs a probability like 0.78, we round it to 1 for Diabetes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "with torch.no_grad():\n",
    "    \n",
    "    test_set_predictions = model(X_test).round()\n",
    "    test_accuracy = sum(test_set_predictions == y_test) / len(y_test)\n",
    "\n",
    "    train_set_predictions = model(X_train).round()\n",
    "    train_accuracy = sum(train_set_predictions == y_train) / len(y_train)\n",
    "    print(f\"Train Accuracy: {train_accuracy.item():.2f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = test_set_predictions.numpy()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pandas.DataFrame(cm, index=[\"True Negative\", \"True Positive\"], columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You have successfully built, trained, and evaluated your first neural network! An accuracy around 75-80% is typical for this dataset with a simple model. \n",
    "\n",
    "**To explore further:**\n",
    "1. Try changing the number of neurons in the hidden layer (from 12 to 32 or 64).\n",
    "2. Try adding a third layer to the network class.\n",
    "3. In the training loop, keep track of the loss on the Test set and see what you can learn from this.\n",
    "4. Try training for more epochs (e.g., 500) and see if the accuracy improves or gets worse (overfitting).\n",
    "5. Compare the performance of the neural network with other statistical models you are already familiar with, e.g. logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
